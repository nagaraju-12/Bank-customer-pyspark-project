
# ğŸ¦ Bank Customer Transaction Insights Using PySpark

This is a mini project demonstrating real-world Data Engineering tasks using PySpark on the Databricks platform. The goal is to simulate data ingestion, cleaning, transformation, and business reporting on 1,00,000 synthetic bank transaction records.

---

## ğŸ“ Project Structure

```
bank-customer-pyspark-project/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ bank_transactions_1L.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_data_cleaning_and_transformation.ipynb
â”‚   â””â”€â”€ 02_transaction_analytics.ipynb
â”œâ”€â”€ output/
â”‚   â””â”€â”€ parquet/
â”‚       â”œâ”€â”€ spend_per_customer/
â”‚       â”œâ”€â”€ top_5_debit_customers/
â”‚       â”œâ”€â”€ monthly_emi_avg/
â”‚       â”œâ”€â”€ emi_user_count/
â”‚       â””â”€â”€ city_avg_txn/
â”œâ”€â”€ README.md
```

---

## ğŸ“Š Project Flow

### âœ… 1. Load Data (from CSV to DataFrame)
- Read the CSV file using PySpark
- Infer schema and display sample records

### âœ… 2. Clean Nulls / Cast Data Types
- Drop rows with critical null values
- Cast all numeric and boolean fields properly

### âœ… 3. Feature Engineering
- Add the following:
  - `HIGH_VALUE_TXN`: Boolean flag if amount > â‚¹50,000
  - `TXN_GROUP`: Categorize into DEBIT, CREDIT, LOAN, OTHER
  - Extract `TXN_YEAR`, `TXN_MONTH`, `TXN_DAY` from TXN_DATE

### âœ… 4. Aggregations
- Total spending per customer
- Monthly average EMI, Loan Repayment, and Bill Payment amounts

### âœ… 5. Save to Parquet
- Save the cleaned data partitioned by `TXN_TYPE`
- Output summaries saved in respective folders

### âœ… 6. Basic Reporting
- Top 5 customers by total **debit** amount
- EMI vs Non-EMI customer counts
- City-wise average transaction value

---

## ğŸ“¦ Technologies Used
- ğŸ PySpark
- ğŸ§± Databricks Community Edition
- ğŸ’¾ Parquet Storage Format
- ğŸ˜ Spark SQL
- ğŸ“ Python (for logic & UDFs)

---

## ğŸ“ Output
All processed files are saved in Parquet under:

```
output/parquet/
â”œâ”€â”€ spend_per_customer/
â”œâ”€â”€ top_5_debit_customers/
â”œâ”€â”€ monthly_emi_avg/
â”œâ”€â”€ emi_user_count/
â””â”€â”€ city_avg_txn/
```

---

## ğŸš€ How to Run
1. Clone this repo
2. Upload `bank_transactions_1L.csv` to Databricks `/FileStore`
3. Run both notebooks in order:
   - `01_data_cleaning_and_transformation.ipynb`
   - `02_transaction_analytics.ipynb`
4. Explore Parquet outputs and verify results

---

## âœï¸ Author
**Nagaraju Chiluveru**  
Aspiring Data Engineer | SQL | PySpark | ETL Developer  
[GitHub](https://github.com/nagaraju-12) | [LinkedIn](https://www.linkedin.com/in/nagaraju-chiluveru-327711236/)

---

## ğŸ License
Free to use for learning and portfolio-building purposes.
